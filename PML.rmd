---
title: "PML_WK4Assignment"
author: "JS"
date: "28 July 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

library(dplyr)
library(lubridate)
library(ggplot2)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(corrplot)

## Get Data

The data for this project come from this source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har. There are 2 CSV files - one with training data and second with testing data.

Based on the description for the assignment and dataset: this is the data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways

Let's load the data

```{r loadData}
data.train<- read.csv("C:/Users/jyoti/Documents/R/PML/pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
data.test<- read.csv("C:/Users/jyoti/Documents/R/PML/pml-testing.csv", na.strings = c("NA", "#DIV/0!", ""))
```

Let's now understand the data and transform : Convert Date and add new variable (Day)
```{r DataUnderstanding}
dim(data.train)
data.train$cvtd_timestamp<- as.Date(data.train$cvtd_timestamp, format = "%m/%d/%Y %H:%M")
data.train$Day<-factor(weekdays(data.train$cvtd_timestamp)) #Add day variable
```


Before I delve deeper, let's figure out what is the goal.

## The Question:

> The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. 
So I need to predict `classe` based on other observations. Measurements directly related to the motion will be more relevant to our model. Based on the column names above - I have numerous observations of the forearm, belt, dumbbell. They are broken down by user, classe, time windown. The main goal is to isolate the movement by how it is done across different people and times. So, the main grouping can be done based on bodypart (as opposed to specific users, timeframes).

## Data Analysis

```{r Analysis}
table(data.train$classe) 
prop.table(table(data.train$classe)) 
prop.table(table(data.train$user_name)) 
prop.table(table(data.train$user_name,data.train$classe),1) 
prop.table(table(data.train$user_name,data.train$classe),2) 
prop.table(table(data.train$classe, data.train$Day),1) 
ggplot2::qplot(x=Day, fill=classe, data = data.train)
```
Key Insights from Exploratory Data Analysis:
### 1.Class-A activity is the most frequently used activity (28.5%) and is most frequently used by user-Jeremy
### 2.Adelmo is the most frequent user of across acitivities (20%) but he uses Class "C" activity most frequently.
### 3.Majority of the actitivies happened during Saturday's and Classes A and B are the most frequently used activites.

## Cleanup

Let's remove `NA` values first, then look at how the movement classes show themselves in the observations. Reload the data with `NA` columns removed for both datasets to keep it consistent. This will narrow down the amount of variables.

```{r cleanup}
#### Remove columns with NA missing values
data.train <- data.train[, colSums(is.na(data.train)) == 0]
data.test <- data.test[, colSums(is.na(data.test)) == 0] 

#### Remove columns that are not relevant to accelerometer measurements.
classe<- data.train$classe
trainRemove<- grepl("^X|timestamp|window", names(data.train))
data.train<- data.train[, !trainRemove]
trainCleaned<- data.train[, sapply(data.train, is.numeric)]
trainCleaned$classe<- classe
testRemove<- grepl("^X|timestamp|window", names(data.test))
data.test<- data.test[, !testRemove]
testCleaned<- data.test[, sapply(data.test, is.numeric)]
```
Now, the cleaned data contains 19622 observations and 53 variables for both train and test datasets

## Create Train and Test data sets:

```{r CreateDataSets}
set.seed(22519)
inTrain <- caret::createDataPartition(trainCleaned$classe, p=0.70, list=F)
trainData <- trainCleaned[inTrain, ]
testData <- trainCleaned[-inTrain, ]
```

## Models

### Indetifying significant variables:
We will fit a predictive model using Random Forest algorithm as it gives important variables and removes multicollinearity  and outliers. We will also use 5-fold cross validation when applying the algorithm.

```{r model}
controlRf <- caret::trainControl(method="cv", 5)
rfmod<- caret::train(classe ~., data=trainData, method="rf", trControl=controlRf, importance=TRUE, ntree=100)
rfmod
```
## Accuracy
### Accuracy of the model on Validation data set
```{r accuracy}
predictRfmod<- predict(rfmod, testData)
confusionMatrix(testData$classe, predictRfmod)
accuracy <- postResample(predictRfmod, testData$classe)
accuracy
Error <- 1 - as.numeric(confusionMatrix(testData$classe, predictRfmod)$overall[1])
Error
```

So, the estimated accuracy of the model is 99.32% and the estimated out-of-sample error is 0.68%.

## Prediction

### Predicting on Test Data Set

```{r Prediction}
result <- predict(rfmod, testCleaned[, -length(names(testCleaned))])
result
```


## Appendix
### Correlation Matrix

```{r CorrelationMatrix}
corrPlot <- cor(trainData[, -length(names(trainData))])
corrplot::corrplot(corrPlot, method="circle")
```
### Tree Visualization
```{r TreeVisualisation}
rtree<- rpart::rpart(classe ~ ., data=trainData, method="class")
rpart.plot::prp(rtree)
```

## Conclusion
As we can we from the result, the random forest algorithem far outperforms the decision tree in terms of accuracy. We are getting 99.32% in sample accuracy while the estimated out-of-sample error is 0.68%.
